{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7a40fd-b2da-461f-8661-a014ec948ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mansour\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2327dfdd-63ad-4faf-bab7-12cddd82f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mansour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Error loading averaged_perceptron_tar: Package\n",
      "[nltk_data]     'averaged_perceptron_tar' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mansour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mansour\\AppData\\Roaming\\nltk_data...\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import streamlit as st \n",
    "# télécharger les ressources nécessaire pour découper  un texte en phrase et en mots (tokénisation)\n",
    "nltk.download('punkt') # nécessaire pour découper un texte en phrase et en mots (tokenisation)\n",
    "nltk.download('averaged_perceptron_taggerr') # nécessaire pour connaitre la nature des mots (nom,verbe,adjectif) = étiquetage \n",
    "nltk.download('stopwords') # liste de mots courants inutile pour l'analyse (le,la,de,et...) , à supprimer du texte \n",
    "nltk.download('wordnet') # dictionnaire lexical pour faire de la lemmatisation (trouver la forme de base des mots )\n",
    "nltk.download('omw-1.4') # nécessaire  pour que WordNet puisse fonctioné correctement \n",
    "\n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.token import word_tokenize , sent_tokenize  # sent_tokenize : utilisé pour découper le texte  en phrase  ,word_tokenize : utilisé pour découper la phrase en mot \n",
    "from nltk.stem import WordNetLemmatizer  #  WordNetLemmatizer permet de faire de la lemmatisation \n",
    "\n",
    "#Chargement du texte (en français ici )\n",
    "with open('pub_entreprise.txt','r',encoding='utf-8') as f :\n",
    "    data=f.read().replace('\\n','')\n",
    "\n",
    "# Découpage (on va  diviser le texte en phrase )\n",
    "sentences=sent_tokenize(data)\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Prétraitement: tokenisation , nettoyage , lemmatisation \n",
    "\n",
    "# Nettoyer une phrase en supprimant le smots inutiles et en ramenant les mots à leur forme de base \n",
    "\n",
    "def preprocess(sentence):\n",
    "\n",
    "    words = word_tokenize(sentence, language='french') # Découpe la phrase en mots en précisant le texte est en français\n",
    "    words = [\n",
    "        word.lower() # word.lower() transforme tous mots en minuscule\n",
    "        for word in words # parcourire la liste des mots\n",
    "        if word.lower() not in stop_words and word not in string.punctuation\n",
    "        #if word.lower() not in stop_word: on enléve les mots vides(comme les articles : le la,est,...)\n",
    "        #and word not in string.punctuation : on enléve aussi la ponctuation(comme :,!,?, ect)\n",
    "        ]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = [\n",
    "          lemmatizer.lemmatize(word)\n",
    "          for word in words\n",
    "          ]\n",
    "\n",
    "\n",
    "\n",
    "    return words\n",
    "\n",
    "corpus=[preprocess(sentence) for sentence in sentences ]# Corpus Prétraité : Une collection de textes ou de phrases , souvent utilisée en traitement  de texte \n",
    "\n",
    "\n",
    "# Fonction  de recherche de la phrase la plus pertinente \n",
    "# Cette fonction de cherche la phrase la plus pertinente du texte par rapport à  une question posée par l'utilisateur (appelée ici query)\n",
    "def get_most_relevant_sentence(query) : # Obtenir la phrase la plus pertinence   \n",
    "\n",
    "    query=preprocess(query) # Query est le paramétre de la fonction . Il représente la question ou  la requete de l'utilisateur \n",
    "    # On utilise la fonction preprocess pour traiter la requete  de l'utilisateur \n",
    "\n",
    "    # On initialise le variable max_similarity à 0 \n",
    "    # Cette variable va stocker le score de similarité maximal  trouvé jusqu'à présent \n",
    "    # Au début , on a encore comparé à aucune phrase \n",
    "    max_similarity=0\n",
    "\n",
    "    #on initialise une variable most_relevant_sentence a une chaine de caractére vide\n",
    "    #cette variable va stocker la phrase du corpus qui a la plus grande similarité avec la requete\n",
    "    #au debut on a pas trouver de phrase pertinente, donc elle est vide \n",
    "    most_relevant_sentence=''\n",
    "\n",
    "    # c'est une boucle for qui parcourt toutes les phrases du corpus\n",
    "    #enumerate(corpus) permet d'obtenir a la fois (i) et la phrase(sentence) a chaque iteration de la boucle \n",
    "    for i, sentence in enumerate(corpus):\n",
    "        similarity=len(set(query).intersection(sentence))/float(len(set(query).union(sentence)))\n",
    "        # set(query) convertit la requete en ensemble de mots uniques\n",
    "        # set(sentence) convertit la phrase courante en un ensemble de mots uniques\n",
    "        # set(query).intersection(sentence): trouve les mots qui sont a la fois dans la requete et dans la phrase\n",
    "        # len(set(query).intersection(sentence)): compte le nombre de mots communs\n",
    "        # set(query).union(sentence): Trouve tous les mots qui sont \n",
    "        # len(set(query).union(sentence)):compte le nombre total de mots uniques dans la requete et la phrase\n",
    "        # la similarité est calculée comme le nombre de mots communs divisé par le nombre total de mots uniques. Cela donne une mesure de la proportion de mots partagés entre la requête et la phrase\n",
    " \n",
    "        #on verifie si la similarité calculée pour la phrase courante est superieure a la similarite maximale trouvée\n",
    "\n",
    "        if similarity> max_similarity :\n",
    "            max_similarity = similarity\n",
    "            most_relevant_sentence = sentences[i]\n",
    "\n",
    "    return most_relevant_sentence\n",
    "\n",
    "\n",
    "# Interface streamlit \n",
    "st.title('chatbot eclat brillant')\n",
    "question=st.text_input('posez des questions sur nos produits: ')\n",
    "if question:\n",
    "    response=get_most_relevant_sentence(question)\n",
    "    st.write('Reponse: ',response)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437270c5-08ac-4bbf-8396-e617d8b15d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be67032-4ad2-42a8-9614-2dee56dbb340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
