{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7a40fd-b2da-461f-8661-a014ec948ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mansour\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mansour\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2327dfdd-63ad-4faf-bab7-12cddd82f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mansour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Error loading averaged_perceptron_tar: Package\n",
      "[nltk_data]     'averaged_perceptron_tar' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mansour\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mansour\\AppData\\Roaming\\nltk_data...\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# télécharger les ressources nécessaire pour découper  un texte en phrase et en mots (tokénisation)\n",
    "nltk.download('punkt') # nécessaire pour découper un texte en phrase et en mots (tokenisation)\n",
    "nltk.download('averaged_perceptron_tar') # nécessaire pour connaitre la nature des mots (nom,verbe,adjectif) = étiquetage \n",
    "nltk.download('stopwords') # liste de mots courants inutile pour l'analyse (le,la,de,et...) , à supprimer du texte \n",
    "nltk.download('wordnet') # dictionnaire lexical pour faire de la lemmatisation (trouver la forme de base des mots )\n",
    "nltk.download('omw-1.4') # nécessaire  pour que WordNet puisse fonctioné correctement \n",
    "\n",
    "import string \n",
    "from nltk.token import word_tokenize , sent_tokenize  # sent_tokenize : utilisé pour découper le texte  en phrase  ,word_tokenize : utilisé pour découper la phrase en mot \n",
    "from nltk.stem import WordNetLemmatizer  #  WordNetLemmatizer permet de faire de la lemmatisation \n",
    "\n",
    "#Chargement du texte (en français ici )\n",
    "with open('pub_entreprise.txt','r',encoding='uft-8') as f :\n",
    "    data=f.read().replace('\\n','')\n",
    "\n",
    "# Découpage (on va  diviser le texte en phrase )\n",
    "sentences=sen_tokenize(data)\n",
    "\n",
    "# Prétraitement: tokenisation , nettoyage , lemmatisation \n",
    "\n",
    "# Nettoyer une phrase en supprimant le smots inutiles et en ramenant les mots à leur forme de base \n",
    "\n",
    "def preprocess(sentence):\n",
    "\n",
    "    words = word_tokenize(sentence, language='french') # Découpe la phrase en mots en précisant le texte est en français \n",
    "    words = [\n",
    "        word.lower() # word.lower() transforme tous mots en minuscule \n",
    "        for word in words # parcourire la liste des mots \n",
    "        if word.lower() not in stow_words and word not in string.punctuation  \n",
    "        #if word.lower() not in stop_word: on enléve les mots vides(comme les articles : le la,est,...)\n",
    "        #and word not in string.punctuation : on enléve aussi la ponctuation(comme :,!,?, ect)\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f753a0-dc3b-48f0-b4d6-eaa844a2ff17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
